{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7a98f0-06bb-4642-b621-bb6f148e0ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2106963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "import numpy as np\n",
    "\n",
    "# Importing dp_transformers modules\n",
    "import dp_transformers\n",
    "from dp_transformers import TrainingArguments as DPTrainingArguments, PrivacyArguments\n",
    "from dp_transformers.dp_utils import OpacusDPTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e6f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f828e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is different than the guide can be removed\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04384b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cb5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28cb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6129441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d483d6473586420cbff5ae85127885cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /model-weights/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/model-weights/Llama-2-7b-hf\",\n",
    "                                                           num_labels=3,\n",
    "                                                           label2id=label2id,\n",
    "                                                           id2label=id2label,\n",
    "                                                           quantization_config=bnb_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac8d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/model-weights/Llama-2-7b-hf\", return_tensors=\"pt\", \n",
    "                                          model_max_length=150)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63bcf89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dataset = load_dataset(\"parquet\", data_files=\"data_balanced.parquet\")\n",
    "\n",
    "dataset = orig_dataset.rename_column(\"Lable\", \"label\").rename_column(\"Text\", \"text\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "toked = dataset.map(preprocess_fn, batched=True).remove_columns(\"text\")\n",
    "\n",
    "def change_labels(example):\n",
    "  label = example[\"label\"]\n",
    "  n_label = 0 if label == \"negative\" else 1 if label == \"neutral\" else 2\n",
    "  example[\"label\"] = n_label\n",
    "  return example\n",
    "\n",
    "toked = toked.map(change_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d986d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'Lable'],\n",
       "        num_rows: 9093\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20eb0610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 910\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772e5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc1e41e-b098-48d1-a780-bcea200a5a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 910\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b74d2fd-9de0-4be5-a36a-e86284e09993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForPrivateWithPadding(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer)\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = super().__call__(examples)\n",
    "\n",
    "\n",
    "        # Huggingface's default way of constructing position_ids is not compatible with Opacus\n",
    "        # since Opacus is not able to deduce the batch size from the input. Here we manually\n",
    "        # generate a position_ids tensor which has the same values as Huggingface's default tensor\n",
    "        # but it is constructed in a way that is compatile with Opacus by using expand_as.\n",
    "        if \"position_ids\" not in batch:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            batch[\"position_ids\"] = torch.arange(\n",
    "                input_ids.shape[1], dtype=torch.long, device=input_ids.device\n",
    "            ).repeat(input_ids.shape[0], 1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a1ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = dp_transformers.DataCollatorForPrivateCausalLanguageModeling(tokenizer)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForPrivateWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e0af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49dba27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    print(f\"eval preds: {eval_pred}\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3c31039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is different than the guide (llama is a bit different, should define pad token)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "932a79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is different than the guide can be removed\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['v_proj', 'down_proj', 'up_proj', 'q_proj', 'gate_proj', 'k_proj', 'o_proj'],\n",
    ")\n",
    "#model.add_adapter(peft_config) can be removed\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6270d1cd-9511-4575-9662-f0de1106cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveScoreCallback(TrainerCallback):  \n",
    "    def __init__(self, model, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_save(self, \n",
    "                args, \n",
    "                state,\n",
    "                control,\n",
    "                **kwargs ):\n",
    "        with open(f\"{args.output_dir}/checkpoint-{state.global_step}/privacy.log\", 'a') as f:\n",
    "            f.write(f\"prv epsilon: {self.trainer.get_prv_epsilon()}\\n\")\n",
    "            f.write(f\"rpd epsilon: {self.trainer.get_rdp_epsilon()}\\n\")\n",
    "        self.model.save_pretrained(f\"{args.output_dir}/checkpoint-{state.global_step}/\")\n",
    "        fname = f\"{args.output_dir}/checkpoint-{state.global_step}/score.original_module.pt\"\n",
    "        torch.save(model.model.score.original_module.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e003331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import dp_transformers\n",
    "import transformers\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "from dp_transformers.grad_sample.transformers import conv_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "108ee8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential Privacy arguments\n",
    "privacy_args = PrivacyArguments(\n",
    "    target_epsilon= 8,\n",
    "    per_sample_max_grad_norm = 1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1edb6-6bf7-4250-b16b-a170791b2394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e0a04e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjusted TrainingArguments for differential privacy\n",
    "dp_training_args = DPTrainingArguments(\n",
    "    output_dir=\"tweet_shield_dp_trained_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    # label_names=[0,1,2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd6bc58-9428-4abd-8192-ec67a76767e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(toked[\"test\"], collate_fn=data_collator,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aeb8961-bc73-4ded-ab01-9f2652627104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, model, eval_dataset, batch_size=6, steps=50):\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_trainer(self, trainer):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.steps == 0:\n",
    "            loss, accuracy = self.compute_metrics()\n",
    "            self.log_metrics(loss, accuracy, state.epoch)\n",
    "            control.should_log = True\n",
    "        \n",
    "    def compute_metrics(self):\n",
    "        with torch.no_grad():\n",
    "            dl = DataLoader(\n",
    "                self.eval_dataset,\n",
    "                collate_fn=DataCollatorForPrivateWithPadding(tokenizer),\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "            \n",
    "            tot_correct = 0\n",
    "            num = 0\n",
    "            sum_loss = 0.0\n",
    "\n",
    "            for batch in tqdm(dl):\n",
    "                batch_size = batch[\"labels\"].shape[0]\n",
    "                num += batch_size\n",
    "                \n",
    "                output = self.model(**batch)\n",
    "    \n",
    "                sum_loss += output[\"loss\"] * batch_size\n",
    "                \n",
    "                probs = self.model(**batch)[\"logits\"].softmax(dim=-1)\n",
    "                preds = torch.argmax(probs, dim=-1)\n",
    "                \n",
    "                tot_correct += (batch[\"labels\"] == preds).sum()\n",
    "\n",
    "\n",
    "            return sum_loss/num, tot_correct/num\n",
    "\n",
    "    \n",
    "    def log_metrics(self, loss, acc, epoch):\n",
    "        logs = {}\n",
    "        logs[\"validation_loss\"] = loss\n",
    "        logs[\"accuracy\"] = acc\n",
    "        print(f\"\"\"validation_loss: {loss}\n",
    "        validation_acc: {acc}\n",
    "        epoch: {epoch}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3445d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 19:31:02:WARNING:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 0/152 [00:00<?, ?it/s]/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 152/152 [01:45<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 2.225940704345703\n",
      "        validation_acc: 0.36043956875801086\n",
      "        epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='438' max='13630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  438/13630 09:22 < 4:43:45, 0.77 it/s, Epoch 0.32/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.444900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_callback = EvaluationCallback(model, toked[\"test\"], steps=1363)\n",
    "\n",
    "trainer = OpacusDPTrainer(\n",
    "# trainer = Trainer(\n",
    "    model=model,\n",
    "    args=dp_training_args,\n",
    "    train_dataset=toked[\"train\"],\n",
    "    # eval_dataset=toked[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    privacy_args=privacy_args,\n",
    "\n",
    ")\n",
    "evaluation_callback.set_trainer(trainer)\n",
    "trainer.add_callback(evaluation_callback)\n",
    "trainer.add_callback(SaveScoreCallback(model, trainer))\n",
    "# ignore_keys = getattr(trainer.model._module.config, \"keys_to_ignore_at_inference\", [])\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd336a-8fb2-4913-803f-c3ca4177f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"prv epsilon: {trainer.get_prv_epsilon()}\")\n",
    "print(f\"rpd epsilon: {trainer.get_rdp_epsilon()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738f90f-fe62-4f16-8ce1-dc9982565739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
