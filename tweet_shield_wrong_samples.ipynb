{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6372678-09a2-4b30-bb8f-3b02233eb1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/fs01/home/dadsetan/condaenvs/lora-finetuning-mia/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c79d26-3496-49c1-a92f-df30db3a20b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacb06d752324e4db76651f39146c58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /model-weights/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "device = \"cuda\"\n",
    "dp_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"/projects/privacyllm/tweet_shield_dp_trained_model/checkpoint-12267\",\n",
    "    num_labels=3,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "dp_model = dp_model.to(device)\n",
    "dp_model.eval()\n",
    "dp_model.config.pad_token_id = dp_model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7c87cf-7cfc-43bf-9fa8-0adb4c4dfb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6673c37b82c4893a39140d3bc8bbebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /model-weights/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"/projects/privacyllm/tweet_shield_normal_trained_model/checkpoint-13640\",\n",
    "    num_labels=3,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "normal_model = normal_model.to(device)\n",
    "normal_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f859acba-a7f7-4f23-bed4-8a9707fd469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/model-weights/Llama-2-7b-hf\", return_tensors=\"pt\", \n",
    "                                          model_max_length=150)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0f3012b-89c0-4a65-8b35-ef8ddc762dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united is changing its  mileage plus program  winners and losers  airline  travel  loyalty  rewards',\n",
       " 'southwest air from lga to san diego',\n",
       " '\"i\\'m so glad there are so many out gay men on a&amp;e\\'s &quot;obsessed.&quot; i relate to russ. this is very difficult to watch. \"',\n",
       " \"southwest air flying flight 3130 tonight at 7 20 from pbi  i have boarding position c 42. is it overbooked? really don't want to be bumped!\",\n",
       " 'southwest air i will direct message you now']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].shuffle()[:5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0497d70d-1d81-4a12-99a4-bf738b90ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dataset = load_dataset(\"parquet\", data_files=\"data_balanced.parquet\")\n",
    "\n",
    "dataset = orig_dataset.rename_column(\"Lable\", \"label\").rename_column(\"Text\", \"text\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# def preprocess_fn(examples):\n",
    "#     return tokenizer(examples[\"text\"], return_tensors='pt')\n",
    "\n",
    "def change_labels(example):\n",
    "  label = example[\"label\"]\n",
    "  n_label = 0 if label == \"negative\" else 1 if label == \"neutral\" else 2\n",
    "  example[\"label\"] = n_label\n",
    "  return example\n",
    "\n",
    "# toked = toked.map(change_labels)\n",
    "\n",
    "# toked = dataset.map(preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b1011-b938-44ed-9c55-9622eecb2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(data, dp_label, normal_label):\n",
    "    print(data[\"text\"])\n",
    "    print(f\"correct label: {id2label[data['label']]}\")\n",
    "    print(f\"dp label:      {id2label[dp_label]}\")\n",
    "    print(f\"normal label:  {id2label[normal_label]}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "both_wrong = []\n",
    "dp_wrong = []\n",
    "normal_wrong = []\n",
    "all_results = []\n",
    "\n",
    "for data in tqdm(toked[\"test\"]):\n",
    "    input_ids = torch.tensor(data[\"input_ids\"]).to(\"cuda\")\n",
    "    \n",
    "    dp_logits = dp_model(input_ids=input_ids)[\"logits\"]\n",
    "    dp_label = torch.argmax(dp_logits,dim=-1)[0].item()\n",
    "    \n",
    "    dp_correct = (dp_label == data[\"label\"])\n",
    "\n",
    "    normal_logits = normal_model(input_ids=input_ids)[\"logits\"]\n",
    "    normal_label = torch.argmax(normal_logits,dim=-1)[0].item()\n",
    "\n",
    "    normal_correct = (normal_label == data[\"label\"])\n",
    "\n",
    "    if normal_correct and dp_correct:\n",
    "        continue\n",
    "\n",
    "    if normal_correct:\n",
    "        dp_wrong.append((data, dp_label, normal_label))\n",
    "    elif dp_correct:\n",
    "        normal_wrong.append((data, dp_label, normal_label))\n",
    "    else:\n",
    "        both_wrong.append((data, dp_label, normal_label))\n",
    "\n",
    "    all_results.append((data, dp_label, normal_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8b2eb-3738-4197-8a64-d58b2460a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(both_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e746008-c3c9-459d-8aa5-f922d94e1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normal_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915402d-5b27-40ec-8aa4-5919c0cf306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dp_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94319b07-6d1e-4383-a1f8-be71f925793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in dp_wrong:\n",
    "    if t[0][\"label\"] == 0:\n",
    "        print_results(*t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bd170-1cd6-4fd2-ac5a-acd61b4b8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad152e-eb8c-4a61-8e19-4fa68eb38b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
